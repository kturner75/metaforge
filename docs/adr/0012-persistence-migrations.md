# ADR-0012: Persistence & Migrations Strategy

## Status
Accepted (Phases 1–3 implemented)

## Context
MetaForge currently uses a single `SQLiteAdapter` that creates tables from entity metadata at startup via `initialize_entity()`. This works for development but has fundamental limitations:

1. **No schema evolution**: Adding a field to entity metadata and restarting the server doesn't alter existing tables — it requires deleting the database and recreating it
2. **No migration history**: There's no record of what schema changes have been applied
3. **SQLite only**: Production deployments need PostgreSQL for concurrent access, reliability, and scalability
4. **No dev/prod parity**: Developing against SQLite and deploying to PostgreSQL risks subtle behavior differences (type coercion, constraint enforcement, JSON handling)

MetaForge's metadata-driven architecture creates a unique migration challenge: the **entity YAML files are the source of truth** for the schema, not hand-written SQL. Migrations need to be derived from metadata changes, not written manually.

### Requirements
- Schema changes flow from metadata YAML → SQL migrations (not the reverse)
- Migrations are deterministic and reviewable (committed to source control)
- Same migration workflow works for SQLite (dev) and PostgreSQL (prod)
- Existing data is preserved across schema changes
- System tables (`_saved_configs`, `_sequences`, etc.) are managed separately from entity tables

## Decision

### Dual Adapter Architecture

We will implement a `PostgreSQLAdapter` alongside the existing `SQLiteAdapter`, both conforming to a shared `PersistenceAdapter` interface:

```python
class PersistenceAdapter(Protocol):
    """Interface all persistence adapters must implement."""

    async def initialize(self) -> None: ...
    async def initialize_entity(self, entity: EntityMetadata) -> None: ...
    async def create(self, entity: str, data: dict) -> dict: ...
    async def get(self, entity: str, id: str) -> dict | None: ...
    async def update(self, entity: str, id: str, data: dict) -> dict: ...
    async def delete(self, entity: str, id: str) -> bool: ...
    async def query(self, entity: str, params: QueryParams) -> QueryResult: ...
    async def aggregate(self, entity: str, params: AggregateParams) -> list[dict]: ...
```

Both adapters use SQLAlchemy Core for query building, ensuring SQL generation is portable. The adapter is selected by configuration:

```python
# config.py
DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///metaforge.db")
# sqlite:///metaforge.db → SQLiteAdapter
# postgresql://user:pass@host/db → PostgreSQLAdapter
```

### Migration Strategy: Metadata-Diff-to-SQL

Migrations are generated by comparing the current metadata against the last-known schema state. This is a **two-step process**: generate, then apply.

#### Step 1: Generate Migration

```bash
metaforge migrate generate --message "add priority field to Contact"
```

This command:
1. Loads current entity metadata from `metadata/entities/*.yaml`
2. Loads the last-known schema snapshot from `migrations/schema_snapshot.json`
3. Computes the diff (new fields, removed fields, type changes, constraint changes)
4. Generates a migration file in `migrations/versions/`
5. Updates the schema snapshot

```
migrations/
  schema_snapshot.json          # last-known schema state (auto-generated)
  versions/
    0001_initial.py             # initial schema creation
    0002_add_priority_to_contact.py
    0003_add_deal_entity.py
```

#### Step 2: Apply Migration

```bash
metaforge migrate apply          # apply all pending migrations
metaforge migrate apply --to 0002  # apply up to a specific version
```

#### Schema Snapshot

The snapshot captures the current state of all entity schemas in a normalized format:

```json
{
  "version": 3,
  "entities": {
    "Contact": {
      "scope": "tenant",
      "fields": {
        "id": { "type": "id", "required": true },
        "fullName": { "type": "name", "required": true, "maxLength": 200 },
        "status": { "type": "picklist", "options": ["active", "inactive"] },
        "priority": { "type": "picklist", "options": ["low", "medium", "high"] }
      }
    }
  },
  "system_tables": {
    "_saved_configs": { "version": 1 },
    "_sequences": { "version": 1 }
  }
}
```

#### Migration File Format

Generated migration files are Python, compatible with Alembic's structure:

```python
"""Add priority field to Contact."""

revision = "0002"
down_revision = "0001"

from alembic import op
import sqlalchemy as sa

def upgrade():
    op.add_column("contact", sa.Column("priority", sa.Text(), nullable=True))

def downgrade():
    op.drop_column("contact", "priority")
```

### Diff Engine: Metadata Change Detection

The diff engine compares the old snapshot against current metadata and produces migration operations:

| Metadata Change | Migration Operation |
|-----------------|-------------------|
| New entity | `CREATE TABLE` with all columns |
| Removed entity | `DROP TABLE` (requires `--allow-destructive` flag) |
| New field | `ADD COLUMN` (nullable by default) |
| Removed field | `DROP COLUMN` (requires `--allow-destructive` flag) |
| Field type change | `ALTER COLUMN TYPE` with cast (warns on data loss risk) |
| New constraint (required) | `ALTER COLUMN SET NOT NULL` (warns if existing nulls) |
| Removed constraint | `ALTER COLUMN DROP NOT NULL` |
| Picklist options added | No schema change (options are metadata, not DB constraints) |
| Picklist options removed | No schema change (warn about orphaned values) |
| New index (unique field) | `CREATE UNIQUE INDEX` |
| Scope change | Add/remove `tenant_id NOT NULL` constraint (complex, requires review) |

#### Safety: Destructive Operations

Destructive operations (drop table, drop column) are **not generated by default**. The developer must explicitly opt in:

```bash
metaforge migrate generate --allow-destructive --message "remove legacy entity"
```

Without this flag, the diff engine warns about the destructive change and generates a no-op with a comment:

```python
def upgrade():
    # WARNING: Entity 'LegacyEntity' was removed from metadata.
    # To drop this table, re-run with --allow-destructive.
    # Existing data will be permanently lost.
    pass
```

### Alembic Integration

We use Alembic as the migration execution engine but **not** its auto-generation. Alembic provides:
- Migration versioning and ordering
- Transaction-safe `upgrade` / `downgrade` execution
- Database version tracking (`alembic_version` table)
- Multi-database support (SQLite + PostgreSQL)

MetaForge's diff engine replaces `alembic revision --autogenerate` because Alembic's autogenerate compares SQLAlchemy models to the database — we need to compare **YAML metadata** to the schema snapshot instead.

```
Standard Alembic:   SQLAlchemy models  ←compare→  Database  →  migration
MetaForge:          YAML metadata  ←compare→  Schema snapshot  →  migration (executed by Alembic)
```

### Dev vs Prod Workflow

| Aspect | Development (SQLite) | Production (PostgreSQL) |
|--------|---------------------|------------------------|
| **Database** | `metaforge.db` local file | PostgreSQL server |
| **Init** | `metaforge migrate apply` | `metaforge migrate apply` |
| **Schema changes** | Generate + apply locally | Generate locally, apply in CI/CD or deploy pipeline |
| **Quick reset** | `metaforge db reset` (drop + recreate) | Not available — migrations only |
| **Seed data** | `metaforge db seed` (dev fixtures) | Separate seed/migration for reference data |

### Parity Checks

A CLI command validates that SQLite and PostgreSQL produce equivalent schemas:

```bash
metaforge migrate check-parity
```

This compares the migration output against both adapters to detect:
- Type mapping differences (e.g., SQLite's loose typing vs PostgreSQL's strict typing)
- Constraint behavior differences
- Index and unique constraint differences
- Default value handling differences

Parity checks run in CI to catch divergences early.

### System Tables

Framework tables (`_saved_configs`, `_sequences`, `_acknowledgment_tokens`, etc.) are managed separately from entity tables:

- System table schemas are defined in code (not YAML)
- System table migrations are bundled with framework upgrades
- They follow the same Alembic migration process but in a separate migration chain
- Entity migrations never touch system tables and vice versa

### CLI Commands

```bash
# Migration management
metaforge migrate generate --message "description"    # generate from metadata diff
metaforge migrate apply                               # apply pending migrations
metaforge migrate apply --to REVISION                  # apply up to specific version
metaforge migrate rollback                             # undo last migration
metaforge migrate status                               # show pending/applied migrations
metaforge migrate check-parity                         # validate SQLite/Postgres equivalence

# Development utilities
metaforge db reset                                     # drop + recreate from migrations (dev only)
metaforge db seed                                      # load development fixtures

# Validation
metaforge metadata validate                            # validate YAML against JSON Schema
metaforge metadata diff                                # show what would change (dry-run of generate)
```

## Consequences

### Positive
- Metadata remains the single source of truth — developers change YAML, not SQL
- Migration files are reviewable in PRs — no surprise schema changes
- Alembic provides battle-tested migration execution and versioning
- Destructive operations require explicit opt-in — data safety by default
- Same migration workflow for dev and prod
- Parity checks catch SQLite/PostgreSQL divergences before they reach production

### Negative
- Custom diff engine is significant upfront work
- Schema snapshot adds a file that must stay in sync (auto-generated, but still a potential source of confusion)
- Some metadata changes don't have clean SQL mappings (e.g., changing a `text` field to `picklist` has no schema impact, but changing `text` to `number` requires a cast)
- Two migration chains (system + entity) add operational complexity

### Risks
- **Data loss on type changes**: Changing a field from `text` to `number` could fail if existing values aren't numeric. Mitigation: the diff engine warns about data-loss-risk operations and generates a migration with explicit cast + fallback.
- **Snapshot drift**: If someone edits migration files manually without updating the snapshot, the next generation produces incorrect diffs. Mitigation: the generate command always regenerates the snapshot from the migration history, not from the previous snapshot file.
- **Complex migrations**: Some changes (splitting a field, merging entities) can't be auto-generated. Mitigation: developers can write custom migration files and mark them in the snapshot. The diff engine skips what's already been migrated.

### Alternatives Considered
- **Alembic autogenerate with SQLAlchemy models**: Generate SQLAlchemy models from metadata, then use Alembic's autogenerate. Rejected: adds an intermediate representation (models) that must be kept in sync with metadata — the diff engine is simpler and more direct.
- **No migrations — recreate on startup**: Always drop and recreate tables from metadata. Rejected: destroys data; only acceptable for dev reset.
- **Database-first with reverse sync**: Write SQL, reverse-engineer metadata. Rejected: contradicts MetaForge's metadata-driven philosophy.
- **Prisma-style declarative migrations**: Define the desired state, let the tool figure out the diff at apply time. Considered but rejected for now: requires the tool to understand the full current database state at apply time, which is more complex than comparing two snapshots. Could revisit if the snapshot approach proves fragile.

### Dependencies
- ADR-0006 (Entity Scoping): scope changes affect table schema (tenant_id column)
- ADR-0009 (Hook System): migration hooks could trigger data transforms post-migration
- Alembic: used as the migration execution engine (already installed in the project)
